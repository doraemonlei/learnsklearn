{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Datasets/MNIST/train.csv')\n",
    "test = pd.read_csv('../Datasets/MNIST/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']\n",
    "x_train = train.drop('label', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:From e:\\workspace\\env_prnet_366\\lib\\site-packages\\skflow\\ops\\losses_ops.py:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: [\"<_RefVariableProcessor(<tf.Variable 'logistic_regression/weights:0' shape=(784, 10) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'logistic_regression/bias:0' shape=(10,) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'learning_rate:0' shape=() dtype=float32_ref>)>\"].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-be42be4e7d2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorFlowLinearClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\workspace\\env_prnet_366\\lib\\site-packages\\skflow\\estimators\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, logdir)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;31m# Sets up model and trainer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m             \u001b[1;31m# Initialize model parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace\\env_prnet_366\\lib\\site-packages\\skflow\\estimators\\base.py\u001b[0m in \u001b[0;36m_setup_training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m             self._trainer = TensorFlowTrainer(\n\u001b[0;32m    150\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                 optimizer=self.optimizer, learning_rate=self.learning_rate)\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;31m# Create model's saver capturing all the nodes created up until now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace\\env_prnet_366\\lib\\site-packages\\skflow\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loss, global_step, optimizer, learning_rate, clip_gradients)\u001b[0m\n\u001b[0;32m     94\u001b[0m         self.trainer = self._optimizer.apply_gradients(grads_and_vars,\n\u001b[0;32m     95\u001b[0m                                                        \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                                                        name=\"train\")\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;31m# Update ops during training, e.g. batch_norm_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'update_ops'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[1;32m--> 586\u001b[1;33m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[0m\u001b[0;32m    587\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: [\"<_RefVariableProcessor(<tf.Variable 'logistic_regression/weights:0' shape=(784, 10) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'logistic_regression/bias:0' shape=(10,) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'learning_rate:0' shape=() dtype=float32_ref>)>\"]."
     ]
    }
   ],
   "source": [
    "classifier = skflow.TensorFlowLinearClassifier(n_classes=10, batch_size=100, steps=1000, learning_rate=0.01)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-fdc2fe65f791>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-fdc2fe65f791>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ```\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "```\n",
    "# import tensorflow as tf\n",
    " \n",
    "# # 获取数据,MNIST数据集包含55000样本的训练集，5000样本的验证集，10000样本的测试集\n",
    "# from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets \n",
    "# mnist = read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    " \n",
    "# # 显示图像和类标的形状\n",
    "# print('训练集信息：')\n",
    "# print(mnist.train.images.shape,mnist.train.labels.shape)\n",
    "# print('测试集信息：')\n",
    "# print(mnist.test.images.shape,mnist.test.labels.shape)\n",
    "# print('验证集信息：')\n",
    "# print(mnist.validation.images.shape,mnist.validation.labels.shape)\n",
    " \n",
    "# # 实现模型 y=softmax(wx+b)\n",
    "# # placeholder：输入数据的地方，None 代表不限条数的输入，每条是784维的向量\n",
    "# # Variable：存储模型参数，持久化的\n",
    "# sess = tf.InteractiveSession()\n",
    "# x = tf.placeholder(tf.float32, [None, 784])\n",
    "# W = tf.Variable(tf.zeros([784,10]))\n",
    "# b = tf.Variable(tf.zeros([10]))\n",
    "# y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    " \n",
    "# # 定义一个交叉熵作为loss函数cross_entropy,其中y是我们预测的概率分布, y_是实际的分布\n",
    "# y_ = tf.placeholder(tf.float32, [None,10])\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    " \n",
    "# # 采用随机梯度下降法，步长为0.5进行训练\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# # 让模型循环训练1000次，每次随机train100条样本\n",
    "# tf.global_variables_initializer().run()\n",
    "# for i in range(1000):\n",
    "#   batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#   train_step.run({x: batch_xs, y_: batch_ys})\n",
    " \n",
    "# # 模型评估\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# print('MNIST手写图片准确率：')\n",
    "# print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tempfile\n",
    "# # import urllib\n",
    "# # train_file = tempfile.NamedTemporaryFile()\n",
    "# # test_file = tempfile.NamedTemporaryFile()\n",
    "\n",
    "# # urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)\n",
    "# # urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# COLUMNS = [\n",
    "#     \"age\", \"workclass\", \"fnlwgt\", \"education\",\n",
    "#     \"education_num\", \"marital_status\", \"occupation\",\n",
    "#     \"relationship\", \"race\", \"gender\", \"capital_gain\",\"capital_loss\",\n",
    "#     \"hours_per_week\", \"native_country\", \"income_bracket\"\n",
    "# ]\n",
    "\n",
    "# df_train = pd.read_csv('../Datasets/Census Income Dataset/adult.data',names=COLUMNS, skipinitialspace=True)\n",
    "\n",
    "# df_test = pd.read_csv('../Datasets/Census Income Dataset/adult.test', names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "\n",
    "\n",
    "# CATEGORICAL_COLUMNS = [\n",
    "#     \"workclass\", \"education\", \"marital_status\",\n",
    "#     \"occupation\", \"relationship\", \"race\",\n",
    "#     \"gender\", \"native_country\"\n",
    "# ]\n",
    "# CONTINUOUS_COLUMNS = [\n",
    "#     \"age\", \"education_num\", \"capital_gain\",\n",
    "#     \"capital_loss\", \"hours_per_week\"\n",
    "# ]\n",
    "# LABEL_COLUMN = \"label\"\n",
    "\n",
    "# def input_fn(df):\n",
    "#     # creates a dictionary mapping from each continuous features\n",
    "#     # column name(k) to the values of that column stored in \n",
    "#     # a constant tensor\n",
    "#     continuous_cols = {\n",
    "#         k:tf.constant(df[k].values)\n",
    "#         for k in CONTINUOUS_COLUMNS\n",
    "#     }\n",
    "#     # creates a dictionary mapping from each categorical\n",
    "#     # feature column mane(k) to the values of that column\n",
    "#     # stored in a tf.SparseTensor\n",
    "\n",
    "#     categorical_cols = {\n",
    "#         k:tf.SparseTensor(\n",
    "#             indices = [[i,0] for i in range(df[k].size)],\n",
    "#             values = df[k].values,\n",
    "#             dense_shape = [df[k].size, 1]\n",
    "#         )\n",
    "#         for k in CATEGORICAL_COLUMNS\n",
    "#     }\n",
    "#     # merges the two dictionaries into one\n",
    "#     feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "#     # converts the label column into a constant tensor\n",
    "#     label = tf.constant(df[LABEL_COLUMN].values)\n",
    "#     # returns the feature columns and the label\n",
    "#     return feature_cols,label\n",
    "\n",
    "# def train_input_fn():\n",
    "#     return input_fn(df_train)\n",
    "# def eval_input_fn():\n",
    "#     return input_fn(df_test)\n",
    "\n",
    "# gender = tf.contrib.layers.sparse_column_with_keys(\n",
    "#     column_name = \"gender\",\n",
    "#     keys = [\"Female\", \"Male\"]\n",
    "# )\n",
    "# education = tf.contrib.layers.sparse_column_with_hash_bucket(\"education\",hash_bucket_size=1000)\n",
    "\n",
    "# relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\"relationship\",hash_bucket_size=100)\n",
    "# workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\"workclass\",hash_bucket_size=100)\n",
    "# occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\"occupation\",hash_bucket_size=1000)\n",
    "# native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\"native_country\",hash_bucket_size=1000)\n",
    "# marital_status = tf.contrib.layers.sparse_column_with_hash_bucket(\"marital_status\",hash_bucket_size=1000)\n",
    "# race = tf.contrib.layers.sparse_column_with_hash_bucket(\"race\",hash_bucket_size=1000)\n",
    "\n",
    "# age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "# education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "# capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "# capital_loss = tf.contrib.layers.real_valued_column(\"captial_loss\")\n",
    "# hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n",
    "\n",
    "# age_buckets = tf.contrib.layers.bucketized_column(age,boundaries=[18,25,30,35,40,45,50,55,60,65])\n",
    "\n",
    "# education_x_occupation = tf.contrib.layers.crossed_column([education,occupation],hash_bucket_size=int(1e4))\n",
    "\n",
    "# age_buckets_x_education_x_occupation=tf.contrib.layers.crossed_column(\n",
    "#     [age_buckets,education,occupation],hash_bucket_size=int(1e6)\n",
    "# )\n",
    "\n",
    "# df_train[LABEL_COLUMN] = (\n",
    "#       df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "# df_test[LABEL_COLUMN] = (\n",
    "#       df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "\n",
    "# model_dir = tempfile.mkdtemp()\n",
    "# m = tf.contrib.learn.LinearClassifier(\n",
    "#     feature_columns = [\n",
    "#         gender, native_country, education, occupation, workclass,\n",
    "#         marital_status, race, age_buckets, education_x_occupation,\n",
    "#         age_buckets_x_education_x_occupation\n",
    "#     ],\n",
    "#     model_dir = model_dir\n",
    "# )\n",
    "\n",
    "# m.fit(input_fn=train_input_fn,steps=200)\n",
    "# results = m.evaluate(input_fn=eval_input_fn,steps=1)\n",
    "# for key in sorted(results):\n",
    "#     print(\"%s: %s\" % (key,results[key]))\n",
    "\n",
    "# m = tf.contrib.learn.LinearClassifier(\n",
    "#     feature_columns=[\n",
    "#         gender, native_country, education, occupation, workclass,\n",
    "#         marital_status, race, age_buckets, education_x_occupation,\n",
    "#         age_buckets_x_education_x_occupation\n",
    "#     ],\n",
    "#     optimizer=tf.train.FtrlOptimizer(\n",
    "#         learning_rate=0.1,\n",
    "#         l1_regularization_strength=1.0,\n",
    "#         l2_regularization_strength=1.0\n",
    "#     ),\n",
    "#     model_dir=model_dir\n",
    "# )\n",
    "# print(\"after regularization!!!!!!!!\")\n",
    "# m1.fit(input_fn=train_input_fn,steps=200)\n",
    "# results1 = m1.evaluate(input_fn=eval_input_fn,steps=1)\n",
    "# for key in sorted(results):\n",
    "#     print(\"%s: %s\" % (key,results1[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# INPUT_NODE = 784\n",
    "# OUTPUT_NODE = 10\n",
    "# GDDOWN = 0.01\n",
    "# TRAINING_TIMES = 1000\n",
    "# TRAINING_STEPS = 100\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "# ##首先打印出mnist图片数据的大小\n",
    "# print(\"input_data\\'s train size: \", mnist.train.num_examples)\n",
    "# print(\"input_data\\'s validation size: \", mnist.validation.num_examples)\n",
    "# print(\"input_data\\'s test size: \", mnist.test.num_examples)\n",
    "# # 各个变量\n",
    "# x = tf.placeholder(\"float\",shape=[None,INPUT_NODE])\n",
    "# y_ = tf.placeholder(\"float\",shape=[None,OUTPUT_NODE])\n",
    "# W = tf.Variable(tf.zeros([INPUT_NODE, OUTPUT_NODE]))\n",
    "# b = tf.Variable(tf.zeros([OUTPUT_NODE]))\n",
    "# # 变量初始化\n",
    "# init = tf.initialize_all_variables()\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(init)\n",
    "# # 初始化图\n",
    "# y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "# # 优化算法\n",
    "# cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "# train_step =tf.train.GradientDescentOptimizer(GDDOWN).minimize(cross_entropy)\n",
    "# for i in range(TRAINING_TIMES):\n",
    "#     # 训练\n",
    "#     batch_xs, batch_ys =mnist.train.next_batch(TRAINING_STEPS)\n",
    "#     sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "#     # 模型评估\n",
    "#     correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "#     accuracy =tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#     j = i+1\n",
    "#     if j%100 == 0:\n",
    "#         print(\"第%d轮训练,训练个数%d个\"% (j,j*TRAINING_STEPS))\n",
    "#         #print(\"正确率预测： \" + correct_prediction + \"\\n\")\n",
    "#         print(\"当前正确率： \")\n",
    "#         print(sess.run(accuracy,feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.1370874 -0.2447794]] [0.06426569]\n",
      "100 [[0.1009016  0.02199123]] [-0.00933374]\n",
      "200 [[0.06803617 0.06174297]] [-0.05327238]\n",
      "300 [[0.06037322 0.07255718]] [-0.0726847]\n",
      "400 [[0.05852193 0.07577487]] [-0.08096339]\n",
      "500 [[0.0580494  0.07682478]] [-0.08443245]\n",
      "600 [[0.05791923 0.07719576]] [-0.08587296]\n",
      "700 [[0.05787996 0.07733477]] [-0.0864682]\n",
      "800 [[0.057867  0.0773889]] [-0.0867136]\n",
      "900 [[0.05786235 0.07741049]] [-0.08681457]\n",
      "0.8742857142857143\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train=pd.read_csv('../Datasets/Breast-Cancer/breast-cancer-train.csv')\n",
    "test=pd.read_csv('../Datasets/Breast-Cancer/breast-cancer-test.csv')\n",
    "\n",
    "x_train=np.float32(train[['Clump Thickness','Cell Size']].T) #读取特征数据之后，为了计算方便，我们进行矩阵转置\n",
    "y_train=np.float32(train['Type'].T)   #训练数据的标记\n",
    "x_test=np.float32(test[['Clump Thickness','Cell Size']].T)\n",
    "y_test=np.float32(test['Type'].T)    \n",
    "\n",
    "b=tf.Variable(tf.zeros([1]))     #线性模型的截距，设置初始值为1.0\n",
    "w=tf.Variable(tf.random_uniform([1,2],-1.0,1.0))#定义一个1*2的矩阵，初始值为-1和1均匀分布的随机数    定义我们的线性函数:\n",
    "y=tf.matmul(w,x_train)+b   # 接着，我们要定义优化目标，通常我们是要让训练值与真实值间的欧氏距离（均方误差）最小，那么代码如下所示：\n",
    "\n",
    "loss=tf.reduce_mean(tf.square(y-y_train)) #tf.redece_mean()是对数据的结果求均值   使用最常见的梯度下降法估计参数w，b,这里我们设置步长为0.01\n",
    "optimizier=tf.train.GradientDescentOptimizer(0.01)\n",
    "train=optimizier.minimize(loss)  #优化loss\n",
    "init=tf.global_variables_initializer()#初始化所有变量参数\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)             #开跑\n",
    "for step in range(1000):             #迭代1000次\n",
    "    sess.run(train)\n",
    "    if step%100==0:\n",
    "        print(step,sess.run(w),sess.run(b))#我们可以看到每100轮迭代之后的w和b值。我们的测试样本一共有175条，那么接下来我们来计算一下训练的这个模型的精度。我们以0.5为界限，精度代码如下。\n",
    "\n",
    "test_true=0\n",
    "for i in range(175):\n",
    "    a=sess.run(w)[0][0]*x_test[0][i]+sess.run(w)[0][0]*x_test[1][i]+sess.run(b)-0.5\n",
    "    if a<0 and y_test[i]==0:\n",
    "        test_true+=1\n",
    "    if a>0 and y_test[i]==1:\n",
    "        test_true+=1\n",
    "print(test_true/175)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-2ce282178b80>:50: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From e:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From e:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From e:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From e:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From e:\\workspace\\env_prnet_366\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Loss: 6.171992301940918\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "def weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    " \n",
    "def biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    " \n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)\n",
    "\n",
    "#import tensorflow as tf\n",
    "# Sandbox\n",
    "# Note: You can't run code in this tab\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#from quiz import weights, biases, linear\n",
    " \n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    " \n",
    "    mnist = input_data.read_data_sets('../MNIST_data/', one_hot=True)\n",
    " \n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    " \n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    " \n",
    "    return mnist_features, mnist_labels\n",
    " \n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    " \n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    " \n",
    "# Weights and Biases\n",
    "w = weights(n_features, n_labels)\n",
    "b = biases(n_labels)\n",
    " \n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    " \n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    " \n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    " \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    " \n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    " \n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    " \n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    " \n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "    \n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
